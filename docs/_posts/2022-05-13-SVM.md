---
layout: single
title:  "Support Vector Machines"
date:   2022-05-13 18:00:00 -0700
categories: 
- Artificial Intelligence
tags:
- Supervised Learning
toc: true
usemathjax: true
---

# Prerequisite

## Hyperplane Definition

A hyperplane can be defined by two vectors $$\vec w = \begin{bmatrix} a \\ b \\ c \end{bmatrix}, \vec x = \begin{bmatrix} x_1 \\ x_2 \\ 1 \end{bmatrix}$$, because

- A hyperplane (2D) can be defined as $$x_2=-\frac{a}{b}x_1-\frac{c}{b}\implies ax_1+bx_2+c=0$$
- And we have $$\vec w^T \vec x=ax_1+bx_2+c$$

## Distance from a point to a line

- Line: $$Ax+By+c=0$$, point: $$(x_0,y_0)$$
- $$d=\frac{|Ax_0+By_0+c|}{\sqrt{A^2+B^2}}$$

## Lagrangian Multipler Method

# Support Vector Machines

<aside>
💡 Intuition: The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space (N - the number of features) that distinctly classifies the data points (the hyperplane with maximum margin).

</aside>

## Problem

- Let $$S=(x_1,y_1),\dots,(x_m,y_m)$$ be a training set of examples, where $$x_i\in\mathbb{R}^d, y_i\in \{+1, -1\}$$
- Lots of linear hyperplanes to classify training set, which one to choose?
- SVM finds the optimal hyperplane that best classifies training set
    
    ![Untitled]({{ site.baseurl }}/assets/images/ML/svm.png)
    

## Terminology

### Decision Hyperplane

- $$\vec w \vec x+b=0$$
    - $$\vec w$$ is a weight vector which is **perpendicular** to the decision plane
        
        ![Untitled]({{ site.baseurl }}/assets/images/ML/svm1.png)
        
        - Proof: say $$\vec x_1,\vec x_2$$ are on the decision plane
        - We have $$\vec w \vec x_1+b=0$$, $$\vec w \vec x_2+b=0$$
        - Without doubt, $$\vec x_1-\vec x_2$$ lies on the decision plane
        - We have  $$\vec w \vec x_1+b-\vec w \vec x_2+b=\vec w (\vec x_1-\vec x_2)=0$$
        - Therefore, vector $$\vec w$$ is perpendicular to the decision plane
    - $$\vec x$$ is input vector
    - $$b$$ is bias

### Support Vectors

- Support vectors are the data points that lie cloest to the decision hyperplane
- They’re **most difficult** to classify
- They have **direct bearing** on the optimum location of the decision hyperplane
    - Delete the support vectors will change the position of the hyperplane.

### Margin

- The minimal distance between a point in the training set and the hyperplane, i.e. the distance from support vectors to the hyperplane
- The street around the hyperplane
- The decision hyperplane is fully specified by a (usually very small) subset of training samples, **the support vectors**.

## Algorithm

### Goal

- Find the hyperplane that maximize the margin
    - 
    
    ![Untitled]({{ site.baseurl }}/assets/images/ML/svm2.png)
    

### Input

- Training set: $$S=(x_1,y_1),\dots,(x_m,y_m)$$, where $$x_i\in\mathbb{R}^d, y_i\in \{+1, -1\}$$

### Output

- A decision hyperplane defined by the weight vector $$\vec w\in\mathbb{R}^{d}$$
- $$\vec w \vec x+b=0$$

### Objective Function

- We want $$\vec w \vec x_i+b\ge 0 \text{ for } y_i=+1, \vec w \vec x_i+b< 0 \text{ for } y_i=-1$$ so that it can classify the training set
    - Combine into $$y_i(\vec w \vec x_i+b)\ge 1$$
- To have a hyperplane with as big a margin as possible
    - Without the loss of generality, for a hyperplane ($$D_1$$ in the diagram) which passes through the positive support vector $$\vec x_+$$ and is parallel to the optimal hyperplane, we have $$\vec w\vec x_++b=+1$$
    - Without the loss of generality, for a hyperplane ($$D_2$$ in the diagram) which passes through the negative support vector $$\vec x_-$$ and is parallel to the optimal hyperplane, we have $$\vec w\vec x_-+b=-1$$
    - According to the distance from a point to a hyperplane, we have margin: $$d=\frac{\lvert\vec w \vec x+b\rvert}{\|\vec w\|}=\frac{1}{\|\vec w\|}$$
        - Alternatively, the margin can be defined by using the dot product
        - $$\begin{split}2d&=\|\vec c\|\times \cos\theta=\frac{\vec c\cdot\vec w}{\|\vec w\|}=(\vec x_+-\vec x_-)\times \frac{\vec w}{\|\vec w\|}\\&=\frac{\vec x_+\vec w}{\|\vec w\|}-\frac{\vec x_+\vec w}{\|\vec w\|}=\frac{1-b}{\|\vec w\|}-\frac{-1-b}{\|\vec w\|}=\frac{2}{\|\vec w\|}\end{split}$$, where $$\vec c$$ is the vector from a negative support vector to a positive support vector
    - In order to **maximize the margin** $$d$$, we need to minimize $$\|\vec w\|$$
    
    ![Untitled]({{ site.baseurl }}/assets/images/ML/svm3.png)
    
- **Final version**
    
    $$
    \min_\vec w \frac{1}{2}\|\vec w\|^2\;\text{ s.t. }\; y_i(\vec w \vec x_i+b)\ge 1 \;\forall i
    $$
    
    - $$\frac{1}{2}$$ and square are used to simplify to derivative
    - $$\frac{1}{2}\|w\|^2$$ is a **quadratic function** whose surface is a **paraboloid**, with just a single global minimum
        
        ![Untitled]({{ site.baseurl }}/assets/images/ML/svm4.png)
        
    - This is **constrained optimization problem** which can be solved by **Lagrange multiplier method**

### Solve with Lagrange Multiplier Method

1. Rewrite original objective function into Lagrangian $$L_p$$
    - Recall the general form of Lagrangian $$L(x,\alpha)=f(x)-\sum_i\alpha_ig_i(x)$$
    - In our case, $$f(x)=\frac{1}{2}\|\vec w\|^2$$
    - $$g(x)=y_i(\vec w \vec x_i+b)-1=0$$ for all training data
    
    $$
    \begin{split}\min_{\vec w, b} L_p&=\frac{1}{2}\|\vec w\|^2-\sum_{i=1}^l\alpha_i[y_i(\vec w \vec x_i+b)-1]\\
    &=\frac{1}{2}\|\vec w\|^2-\sum_{i=1}^l\alpha_iy_i(\vec w \vec x_i+b)+\sum_{i=1}^l\alpha_i
    \end{split}
    $$
    
    - where $$l$$ is the number of training points
2. To get the min, we need to find the tangent point whose partial derivatives equals 0
    - $$\begin{split}\frac{\partial L_p}{\partial \vec w}&=\vec w-\sum_{i=1}^l\alpha_iy_i\vec x_i=0\\\implies \vec w&=\sum_{i=1}^l\alpha_iy_i\vec x_i\end{split}$$
    - $$\frac{\partial L_p}{b}=\sum_{i=1}^l\alpha_iy_i=0$$
3. Convert the **primal** form of the optimization problem to the **dual** form
    - The Lagrangian Dual Problem: instead of **minimizing** over $$\vec w, b$$, subject to constraints involving $$\alpha_i$$, we can **maximize** over $$\alpha_i$$ (the dual variable) subject to the relations obtained previously for  $$\vec w, b$$
    - Our solution must satisfy these **two relations**
        - $$\vec w=\sum_{i=1}^l\alpha_iy_i\vec x_i$$
        - $$\sum_{i=1}^l\alpha_iy_i=0$$
    - By substituting for  $$\vec w, b$$ back in the original equation, we can get rid of the dependence on   $$\vec w, b$$
        
        $$
        \begin{equation}
        \begin{split}
        L_p&=\frac{1}{2}\|\vec w\|_2^2-\sum_{i=1}^l\alpha_iy_i(\vec w \vec x_i+b)+\sum_{i=1}^l\alpha_i\; \text{ (primal form) }\\
        &=\frac{1}{2}\|\vec w\|_2^2-\vec w\sum_{i=1}^l\alpha_iy_i\vec x_i -b\sum\alpha_iy_i+\sum_{i=1}^l\alpha_i\\
        &=\frac{1}{2}\vec w^T\vec w-\vec w^T\vec w-b\times 0+\sum_{i=1}^l\alpha_i\; \text{(by substituting two relations)}\\
        &=\sum_{i=1}^l\alpha_i-\frac{1}{2}\vec w^T\vec w\;\text{(by substituting w)}\\
        &=\sum_{i=1}^l\alpha_i-\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_jy_iy_j(\vec x_i\vec x_j)\\
        \end{split}
        \end{equation}
        $$
        
    - We will now solve for the $$\alpha_i$$ by differentiating the dual problem wrt $$\alpha_i$$, and setting it to zero. Most of the $$\alpha_i$$ will turn out to have the value zero. The non-zero $$\alpha_i$$ will
    correspond to the **support vectors**.

### Dual Problem

$$
\begin{equation}
\begin{split}
\min_{\vec w, b} L_p=&\sum\alpha_i-\frac{1}{2}\sum_{i=1}^l\alpha_i\alpha_jy_iy_j(\vec x_i\vec x_j)\\ \iff
\max_{\alpha_i}L_D(\alpha_i)=&\sum\alpha_i-\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_jy_iy_j(\vec x_i\vec x_j)\; \text{ (dual form) }\\
&\text{s.t.} \sum_i \alpha_iy_i=0 \;\&\; \alpha_i\ge 0
\end{split}
\end{equation}
$$

- Note we have removed the dependence on $$\vec w, b$$
    - Note first that we already now have our answer for what the weights $$\vec w$$ ****must be: they are a linear combination of the training inputs and the training outputs, $$x_i, y_i$$ **and the values of $$\alpha_i$$*.*
- Kuhn-Tucker theorem: the solution we find here will be the same as the solution to the original problem.
- **Why are we solving the dual problem instead of the original problem?**
    - Because this will let us solve the problem by computing the just the inner products of $$x_i, x_j$$ **(which will be very important later on when we want tosolve non-linearly separable classification problems)
- To solve the dual problem, we just need to take the derivative wrt $$\alpha_i$$ and set it equal to 0
- After solving $$\alpha_i$$, we can find the weight $$\vec w$$ by using $$\vec w=\sum_{i=1}^l\alpha_iy_i\vec x_i$$
    - Most of the weights $$\vec w_i$$, i.e, the $$\alpha_i$$ will be zero. Only the support vectors will have nonzero weights or $$\alpha_i$$. — This reduces the dimensionality of the solution.

# Soft Margin SVM

## Why soft margin?

- Hard margin: all data are separated correctly
- Soft margin: allow some margin violation to occur
- It’s not always plausible to classify all the data points correctly with a hyperplane, so we need to tolerate some error classification which doesn’t satisfie the restriction: $$y_i ( w^T \vec x_i + b ) \ge 1$$. We call this soft margin SVM

## Objective Function

1. To address the problem of nonseparable datasets, as well as sensitivity to outliers, we introduce slack variables $$\xi$$ in the constraint.
    1. Before: $$y_i ( w^T \vec x_i + b ) \ge 1$$
    2. Now: $$y_i ( w^T \vec x_i + b ) \ge 1-\xi_i,\quad\xi_i\ge0$$
    3. Think $$\xi_i$$ as the distance from the point to the correct decision boundry
        1. When the point is on the correct side and has margin ≥ 1, $$\xi_i=0$$
        2. When the point is on the correct side but falls within margin, $$0<\xi_i\le1$$
        3. When the point is on the wrong side, $$\xi_i>1$$
        
        ![soft-margin]({{ site.baseurl }}/assets/images/ML/svm-softmargin.png)
        
2. We add a penalty proportional to the amount by which the example is misclassified $$C\xi_i$$
    - The parameter $$C$$ controls the relative weighting between the goal of making the margin small and ensuring that most examples have functional margins that are at least 1.
    - When $$C=+\infty$$, objective function force all the data points subject to our restriction $$y_i ( w^T \vec x_i + b ) \ge 1$$.
    - Otherwise, the function tolerates some error classification.
    - In practice, C is determined by cross validation. Note only that using 1 by default does not work very well, while $$\frac{1}{l}$$ may work better.
3. The optimization problem now becomes
    
    $$
    \min_\vec w \frac{1}{2}\|\vec w\|^2+C\sum_{i=1}^l\xi_i,\\ s.t.\quad y_i ( w^T \vec x_i + b ) \ge 1-\xi_i,\quad\xi_i\ge0
    $$
    
4. Variant.
    
    $$
    \min_\vec w \frac{1}{2}\|\vec w\|^2+C\sum_{i=1}^l\ell_{0/1}(y_i(\vec w \vec x_i+b)),\;\ell_{0/1}(z)=\begin{cases}1,\text{ if }z<0\\0,\text{ otherwise}\end{cases}
    $$
    
    - Because $$\ell_{0/1}(z)$$ is not a convex and continuous function, we usually use other function to take place of  $$\ell_{0/1}(z)$$ we call them “surrogate loss”.
        - hinge loss: $$\ell_{hinge}(z)=\max(0,1-z)$$
        - exponential loss: $$\ell_{exp}(z)=\exp(-z)$$
        - logistic loss: $$\ell_{hinge}(z)=\log (1+\exp{(-z)})$$

# Kernel

## Problem

- What if the data points are not linear separable?
    
    ![Untitled]({{ site.baseurl }}/assets/images/ML/svm5.png)
    

## Non-Linear SVMs

### Intuition

- The idea is to gain linearly separation by mapping the data to a higher dimensional space.
    
    ![Untitled]({{ site.baseurl }}/assets/images/ML/svm6.png)
    
- For example, the following set can’t be separated by a linear function, but can be separated by a quadratic one
    
    ![Untitled]({{ site.baseurl }}/assets/images/ML/svm7.png)
    

### Kernel

- Why kernel?
    - Recall the dual objective function: $$\max_{\alpha_i}L_D(\alpha_i)=\sum\alpha_i-\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_jy_iy_j(\vec x_i\vec x_j)$$, we need to compute the inner product of $$\vec x_i\vec x_j$$ for every pair
    - To map data points to higher dimensional space, we have to compute the inner product of $$\phi(\vec x_i)\phi(\vec x_j)$$ where function $$\phi$$ is the mapping basis.
        - This is expensive and time consuming (suppose $$\phi$$ is a quartic polynomial or worse)
    - Instead, we find a “**kernel function**” K such that $$K(\vec x_i,\vec x_j)=\phi(\vec x_i)\phi(\vec x_j)$$, then we don’t need to compute $$\phi$$ at all.
        - The kernel function defines **inner products (or similarity)** in the mapped space.
- Example
    - The four 2D points are not linear separable
        
        ![Untitled]({{ site.baseurl }}/assets/images/ML/svm8.png)
        
    - With polynomial kernel $$K(\vec x_1,\vec x_2)=(\vec x_1^T\vec x_2)^2=(x_1x_2+y_1y_2)^2=x_1^2x_2^2+y_1^2y_2^2+2x_1x_2y_1y_2$$
    - We can find corresponding basis function such that $$K(\vec x_i,\vec x_j)=\phi(\vec x_i)\phi(\vec x_j)$$
        - $$\phi(\vec x_i)=[x_i^2,y_i^2, \sqrt2 x_iy_i]$$
        - $$\phi(\vec x_1)\phi(\vec x_2)=x_1^2x_2^2+y_1^2y_2^2+2x_1x_2y_1y_2=K(\vec x_1,\vec x_2)$$
        - The data points in transformed space can be linearly separable
            
            ![Untitled]({{ site.baseurl }}/assets/images/ML/svm9.png)
            

# Appendix

## Inner Products

- Why should inner product be involved in classification?
    - Intuition is that inner products provide some measure of ‘similarity’
    - If two vectors are parallel, their inner product is 1 (completely similar)
    - If two vectors are perpendicular, their inner product is 0 (completely unlike)
- In SVM dual problem, we have $$\max_{\alpha_i}L_D(\alpha_i)=\sum\alpha_i-\frac{1}{2}\sum_{i=1}^l\sum_{j=1}^l\alpha_i\alpha_jy_iy_j(\vec x_i\vec x_j)$$.
    - The function will be maximized if we give nonzero values to $$\alpha_i$$ that correspond to the support vectors. Why?
    - Case 1: If two data points $$\vec x_i,\vec x_j$$ are completely **dissimilar** (**orthogonal**), their inner product is **0**, and they don’t contribute to $$L_D$$
        
        ![Untitled]({{ site.baseurl }}/assets/images/ML/svm10.png)
        
    - Case 2: If two data points $$\vec x_i,\vec x_j$$ are **alike**, their inner product is most likely greater than 0
        - If both  $$\vec x_i,\vec x_j$$ belong to the same class, then  $$y_i,y_j$$ are both either +1 or -1. Thus the value of $$\alpha_i\alpha_jy_iy_j(\vec x_i\vec x_j)$$ will be positive, this would decrease the value of L (since we are subtracting this term). So, the algorithm downgrades similar data points that belong to the same class.
            
            ![Untitled]({{ site.baseurl }}/assets/images/ML/svm11.png)
            
        - If both  $$\vec x_i,\vec x_j$$ belong to different classes, then  $$y_i,y_j$$ have different sign. Thus the value of $$\alpha_i\alpha_jy_iy_j(\vec x_i\vec x_j)$$ will be negative, this would increase the value of L (since we are subtracting this term). So, the algorithm placees high weights on the critical data points that tell the two classes aprt (support vectors).
            
            ![Untitled]({{ site.baseurl }}/assets/images/ML/svm12.png)
            
        
# Reference
- [An Idiot’s guide to Support vector machines (SVMs)](https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf)