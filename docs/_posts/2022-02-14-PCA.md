---
layout: single
title:  "Principle Component Analysis"
date:   2022-02-14 18:00:00 -0700
categories: 
- Artificial Intelligence
tags:
- Dimension Reduction
toc: true
usemathjax: true
toc_sticky: true
---


ðŸ’¡ Use eigenvectors of ***Cor(X)*** to create new features which are linear combinations of original features. Generated Principle Components are ordering from largest **variance** to lowest **variance** according to magnitude of corresponding **eigenvalues**.

## Overview

- For a given variable, higher variance means more informative.
- Principal Components
    - Are new variables that are constructed as linear combinations of the initial variables.
    - Are calculated in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components.
- With PCA, we can achieve **dimension reduction**, which leads to
    - Avoid overfitting
    - Reduce computation

## Inference

1. **We are interested in finding new variables that are linear combinations of our original features that maximize variance** (higher variance means more informative)
    
    ![Untitled]({{ site.baseurl }}/assets/images/ML/pca.png)
    
2. Formally, we want to choose $$u$$ that maximize $$Var(u'X)=Cov(u'X)=u'Cov(X)u$$ with constraint of norm1, i.e. $$uâ€™u=1$$ 
    - Why constraint? Larger ***u*** gives large ***Cov(uâ€™X)***, but we canâ€™t make u infinitely large.
3. With Lagrange multipliers, we redefine the problem as maximize $$u'Cov(X)u-w(uâ€™u-1)$$ with respect to ***u, w***
4. Then by taking its derivative to 0, we have $$2Cov(X)u-2wu=0\implies Cov(X)u=wu$$
5. In other words, ***u*** is an eigenvector of ***Cov(X)***.
6. Then for another linear combination $$u_2$$, we also want to maximize its variance and make it uncorrelated with $$u$$.
7. By adding a new constriant $$u'u_2=0$$ (leading to $$Cov(uâ€™X, u_2â€™X)=0$$)
8. With Lagrange multipliers, we can also solve it and find it also an eigenvector.
9. Therefore, normalized (make sure $$AA'=I$$) eigenvectors are linear combinations weâ€™re exactly looking for.

## Skeleton

1. Say we are given a dataset ***X (p x n)*** with ***n*** data points of ***p*** variables, calculate covariance matrix ***Cov(X) (p x p)***
2. Calculate eigenvalue decomposition of ***Cov(X), i.e.*** $$Cov(X)=A\Sigma A^{-1}$$
    - Eigenvalue $$\lambda$$, Eigenvector $$\vec{a}$$ are what hold $$\vec{Xa}=\lambda\vec{a}$$.
    - ***A*** is a square matrix ***(p x p)*** holding eigenvectors $$\vec{a_i}$$ in columns.
    - $$\Sigma$$ is a diagonal matrix ***(p x p)*** with eigenvalues $$\lambda_i$$ along the diagonal in descending order.
    - Because ***Cov(X)*** is symmetric, and any symmetric matrix has real (not complex) eigenvalues, and that the corresponding eigenvectors $$\vec{a}_1, ..., \vec{a}_p$$ are orthogonal. And since we will normalize eigenvectors, We have $$AA'=I\implies A'=A^{-1}$$
3. Reorder eigenvectors so that corresponding eigenvalues in descending order
4. Principle Components can be calculated as $$\vec{w}_i=\vec{a}_i'\vec{x}\implies W=A'\vec{x}$$
    - Principle Components are uncorrelated to each other.
        - Because of $$Cov(AX) = ACov(X)A'$$ [property](https://courses.washington.edu/b533/lect3.pdf) of covariance matrix, we have $$Cov(W)=Cov(A'X)=A'Cov(X)A$$
        - Because $$Cov(X)=A\Sigma A^{-1}$$ and $$A'=A^{-1}$$, we have $$Cov(W)=A^{-1}A\Sigma A^{-1}A=\Sigma$$, a diagonal matrix.
        - Because ***Cov(W)*** is a diagonal matrix, we have $$Cov(\vec{w}_i,\vec{w}_j)=0$$ when iâ‰ j, therefore uncorrelated.
    - Principle Components are ordered as $$Var(\vec{w}_1)\geq Var(\vec{w}_2)\geq \ldots \geq Var(\vec{w}_p)$$
        - The eigenvalues represent the magnitude of the spread in the direction of the eigenvectors.
5. Choose a hyperparameter ***s***, use the first ***s*** eigenvectors to form ***U***, then new feature matrix ***W=UX***

## Scaling

- Motivation: If we apply PCA on original data, variables with large variance will donimate, in which units play a role, e.g. distances with meter matters more than with kilometers.
- Solution: Scaling. For each column, substract the column mean and divide by the standard deviation.
    - Pros: alleviate side effects from units on variance
    - Cons: all PCA generated variables have variance 1.
    - Alternative: Divide each column by its mean.